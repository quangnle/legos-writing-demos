// ===========================
// 1. CÃC BIáº¾N TOÃ€N Cá»¤C
// ===========================
let maxRewardSoFar = 0;       // Äiá»ƒm sá»‘ (reward) cao nháº¥t Ä‘áº¡t Ä‘Æ°á»£c trong quÃ¡ trÃ¬nh training
let bestEpisodeStates = [];   // Máº£ng lÆ°u trá»¯ toÃ n bá»™ cÃ¡c tráº¡ng thÃ¡i (states) cá»§a episode tá»‘t nháº¥t
                               // Má»—i pháº§n tá»­ lÃ  má»™t máº£ng [x, x_dot, theta, theta_dot]
                               // DÃ¹ng Ä‘á»ƒ chiáº¿u láº¡i (replay) episode ká»· lá»¥c
let bestModel = null;         // MÃ´ hÃ¬nh tá»‘t nháº¥t (policy tá»‘t nháº¥t) - Ä‘Æ°á»£c lÆ°u khi phÃ¡ ká»· lá»¥c
                               // DÃ¹ng Ä‘á»ƒ chÆ¡i thá»­ vá»›i policy tá»‘t nháº¥t thay vÃ¬ policy hiá»‡n táº¡i
let isReplaying = false;      // Cá» boolean Ä‘Ã¡nh dáº¥u Ä‘ang trong quÃ¡ trÃ¬nh chiáº¿u láº¡i episode tá»‘t nháº¥t
                               // Khi Ä‘ang replay thÃ¬ sáº½ khÃ´ng cháº¡y training Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t
let isTraining = true;        // Cá» boolean Ä‘iá»u khiá»ƒn viá»‡c training cÃ³ Ä‘ang cháº¡y hay khÃ´ng
                               // CÃ³ thá»ƒ táº¡m dá»«ng/resume thÃ´ng qua UI
let isTestPlaying = false;    // Cá» boolean Ä‘Ã¡nh dáº¥u Ä‘ang trong cháº¿ Ä‘á»™ chÆ¡i thá»­ (test play)
                               // Khi Ä‘ang test play thÃ¬ sáº½ khÃ´ng cháº¡y training Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t
let episodeCount = 0;         // Bá»™ Ä‘áº¿m sá»‘ lÆ°á»£ng episode Ä‘Ã£ cháº¡y tá»« khi khá»Ÿi Ä‘á»™ng

// ===========================
// 2. MÃ”I TRÆ¯á»œNG CART-POLE & Máº NG NEURAL NETWORK
// ===========================

/**
 * Class mÃ´ phá»ng mÃ´i trÆ°á»ng CartPole (Xe Ä‘áº©y vá»›i cÃ¢y gáº­y)
 * ÄÃ¢y lÃ  bÃ i toÃ¡n cá»• Ä‘iá»ƒn trong Reinforcement Learning
 */
class CartPole {
    /**
     * Constructor khá»Ÿi táº¡o cÃ¡c tham sá»‘ váº­t lÃ½ cá»§a mÃ´i trÆ°á»ng
     */
    constructor() {
        this.gravity = 9.8;        // Gia tá»‘c trá»ng trÆ°á»ng (m/sÂ²)
        this.massCart = 1.0;       // Khá»‘i lÆ°á»£ng cá»§a xe Ä‘áº©y (kg)
        this.massPole = 0.1;       // Khá»‘i lÆ°á»£ng cá»§a cÃ¢y gáº­y (kg)
        this.totalMass = this.massCart + this.massPole;  // Tá»•ng khá»‘i lÆ°á»£ng
        this.length = 0.5;         // Chiá»u dÃ i má»™t ná»­a cá»§a cÃ¢y gáº­y (m)
        this.poleMoment = this.massPole * this.length;   // Momen quÃ¡n tÃ­nh cá»§a gáº­y
        this.forceMag = 10.0;      // Äá»™ lá»›n lá»±c Ä‘áº©y tá»‘i Ä‘a cÃ³ thá»ƒ Ã¡p dá»¥ng (N)
        this.tau = 0.02;           // BÆ°á»›c thá»i gian cho má»—i step (s) - 0.02s = 50Hz
        this.reset();              // Khá»Ÿi táº¡o tráº¡ng thÃ¡i ban Ä‘áº§u
    }

    /**
     * Reset mÃ´i trÆ°á»ng vá» tráº¡ng thÃ¡i ban Ä‘áº§u
     * @returns {Array} Máº£ng chá»©a 4 giÃ¡ trá»‹ tráº¡ng thÃ¡i: [x, x_dot, theta, theta_dot]
     *   - x: vá»‹ trÃ­ ngang cá»§a xe (m), khoáº£ng [-2.4, 2.4]
     *   - x_dot: váº­n tá»‘c ngang cá»§a xe (m/s)
     *   - theta: gÃ³c nghiÃªng cá»§a gáº­y (radian), khoáº£ng [-0.209, 0.209] (~Â±12 Ä‘á»™)
     *   - theta_dot: váº­n tá»‘c gÃ³c cá»§a gáº­y (radian/s)
     */
    reset() {
        // Khá»Ÿi táº¡o vá»›i giÃ¡ trá»‹ ngáº«u nhiÃªn nhá» gáº§n trung tÃ¢m Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh á»•n Ä‘á»‹nh
        this.state = [
            Math.random()*0.1-0.05,    // x: ngáº«u nhiÃªn trong [-0.05, 0.05]
            Math.random()*0.1-0.05,    // x_dot: váº­n tá»‘c ngang ban Ä‘áº§u nhá»
            Math.random()*0.1-0.05,    // theta: gÃ³c nghiÃªng ban Ä‘áº§u nhá»
            Math.random()*0.1-0.05     // theta_dot: váº­n tá»‘c gÃ³c ban Ä‘áº§u nhá»
        ];
        this.done = false;  // Cá» bÃ¡o hiá»‡u episode chÆ°a káº¿t thÃºc
        return this.state;
    }

    /**
     * Thá»±c hiá»‡n má»™t bÆ°á»›c trong mÃ´i trÆ°á»ng
     * @param {number} action - HÃ nh Ä‘á»™ng: 0 = Ä‘áº©y sang trÃ¡i, 1 = Ä‘áº©y sang pháº£i
     * @returns {Object} Object chá»©a:
     *   - state: Tráº¡ng thÃ¡i má»›i sau khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng
     *   - reward: Äiá»ƒm thÆ°á»Ÿng (1 náº¿u chÆ°a fail, 0 náº¿u Ä‘Ã£ fail)
     *   - done: Boolean cho biáº¿t episode Ä‘Ã£ káº¿t thÃºc chÆ°a
     */
    step(action) {
        // Giáº£i nÃ©n tráº¡ng thÃ¡i hiá»‡n táº¡i
        let [x, x_dot, theta, theta_dot] = this.state;
        
        // XÃ¡c Ä‘á»‹nh lá»±c Ä‘áº©y dá»±a trÃªn hÃ nh Ä‘á»™ng
        // action = 1: Ä‘áº©y sang pháº£i (lá»±c dÆ°Æ¡ng), action = 0: Ä‘áº©y sang trÃ¡i (lá»±c Ã¢m)
        const force = action === 1 ? this.forceMag : -this.forceMag;
        
        // TÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ lÆ°á»£ng giÃ¡c cáº§n thiáº¿t
        const costheta = Math.cos(theta);
        const sintheta = Math.sin(theta);
        
        // TÃ­nh gia tá»‘c táº¡m thá»i (theo cÃ´ng thá»©c váº­t lÃ½)
        // CÃ´ng thá»©c nÃ y dá»±a trÃªn phÆ°Æ¡ng trÃ¬nh chuyá»ƒn Ä‘á»™ng cá»§a há»‡ thá»‘ng cart-pole
        const temp = (force + this.poleMoment * theta_dot * theta_dot * sintheta) / this.totalMass;
        
        // TÃ­nh gia tá»‘c gÃ³c cá»§a gáº­y (thetaacc)
        // ÄÃ¢y lÃ  phÆ°Æ¡ng trÃ¬nh vi phÃ¢n báº­c hai mÃ´ táº£ chuyá»ƒn Ä‘á»™ng cá»§a con láº¯c ngÆ°á»£c
        const thetaacc = (this.gravity * sintheta - costheta * temp) / 
                        (this.length * (4.0/3.0 - this.massPole * costheta * costheta / this.totalMass));
        
        // TÃ­nh gia tá»‘c ngang cá»§a xe (xacc)
        const xacc = temp - this.poleMoment * thetaacc * costheta / this.totalMass;
        
        // Cáº­p nháº­t tráº¡ng thÃ¡i báº±ng phÆ°Æ¡ng phÃ¡p Euler (tÃ­ch phÃ¢n sá»‘)
        // Sá»­ dá»¥ng cÃ´ng thá»©c: vá»‹_trÃ­_má»›i = vá»‹_trÃ­_cÅ© + váº­n_tá»‘c * delta_t
        x += this.tau * x_dot;
        x_dot += this.tau * xacc;
        theta += this.tau * theta_dot;
        theta_dot += this.tau * thetaacc;
        
        // LÆ°u tráº¡ng thÃ¡i má»›i
        this.state = [x, x_dot, theta, theta_dot];
        
        // Kiá»ƒm tra Ä‘iá»u kiá»‡n káº¿t thÃºc episode (fail conditions)
        // Episode káº¿t thÃºc náº¿u:
        // - Xe di chuyá»ƒn quÃ¡ xa: |x| > 2.4m
        // - Gáº­y nghiÃªng quÃ¡ nhiá»u: |theta| > 0.209 radian (~12 Ä‘á»™)
        this.done = x < -2.4 || x > 2.4 || theta < -0.209 || theta > 0.209;
        
        // Reward = 1 náº¿u chÆ°a fail, 0 náº¿u Ä‘Ã£ fail
        // Má»¥c tiÃªu lÃ  giá»¯ gáº­y tháº³ng Ä‘á»©ng cÃ ng lÃ¢u cÃ ng tá»‘t
        const reward = this.done ? 0 : 1;
        
        return { state: this.state, reward, done: this.done };
    }
}

// ===========================
// Cáº¤U HÃŒNH Máº NG NEURAL NETWORK (ACTOR-CRITIC)
// ===========================
const numInputs = 4;           // Sá»‘ Ä‘áº§u vÃ o: 4 giÃ¡ trá»‹ tráº¡ng thÃ¡i [x, x_dot, theta, theta_dot] 
                               // tÆ°Æ¡ng á»©ng vá»›i 4 sensor cá»§a xe Ä‘áº©y bao gá»“m vá»‹ trÃ­ ngang, váº­n tá»‘c ngang, gÃ³c nghiÃªng vÃ  váº­n tá»‘c gÃ³c cá»§a gáº­y    
const numActions = 2;          // Sá»‘ hÃ nh Ä‘á»™ng: 0 (trÃ¡i) vÃ  1 (pháº£i)
                               // tÆ°Æ¡ng á»©ng vá»›i 2 hÃ nh Ä‘á»™ng: Ä‘áº©y sang trÃ¡i vÃ  Ä‘áº©y sang pháº£i
const hiddenSize = 128;        // Sá»‘ neuron trong lá»›p áº©n
const learningRate = 0.01;     // Tá»‘c Ä‘á»™ há»c (learning rate) cho optimizer
const gamma = 0.99;            // Há»‡ sá»‘ chiáº¿t kháº¥u (discount factor) cho reward tÆ°Æ¡ng lai

/**
 * Táº¡o mÃ´ hÃ¬nh Actor-Critic sá»­ dá»¥ng TensorFlow.js
 * Actor-Critic káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a Policy Gradient vÃ  Value-based methods
 * 
 * - Actor: Há»c policy (xÃ¡c suáº¥t chá»n má»—i hÃ nh Ä‘á»™ng)
 * - Critic: ÄÃ¡nh giÃ¡ giÃ¡ trá»‹ (value) cá»§a tráº¡ng thÃ¡i hiá»‡n táº¡i
 * 
 * @returns {tf.Model} MÃ´ hÃ¬nh cÃ³ 2 Ä‘áº§u ra: [actor_output, critic_output]
 */
function createActorCriticModel() {
    // Lá»›p Ä‘áº§u vÃ o: nháº­n 4 giÃ¡ trá»‹ tráº¡ng thÃ¡i
    const input = tf.input({shape: [numInputs]});
    
    // Lá»›p áº©n: Dense layer vá»›i 128 neurons, dÃ¹ng ReLU activation
    // ReLU giÃºp máº¡ng há»c cÃ¡c hÃ m phi tuyáº¿n tÃ­nh
    const dense = tf.layers.dense({units: hiddenSize, activation: 'relu'}).apply(input);
    
    // Actor head: Output xÃ¡c suáº¥t cho má»—i hÃ nh Ä‘á»™ng (softmax Ä‘á»ƒ tá»•ng = 1)
    // Softmax Ä‘áº£m báº£o phÃ¢n phá»‘i xÃ¡c suáº¥t há»£p lá»‡: p(action=0) + p(action=1) = 1
    const actor = tf.layers.dense({units: numActions, activation: 'softmax'}).apply(dense);
    
    // Critic head: Output giÃ¡ trá»‹ ká»³ vá»ng (expected value) cá»§a tráº¡ng thÃ¡i
    // KhÃ´ng dÃ¹ng activation, output lÃ  sá»‘ thá»±c (cÃ³ thá»ƒ Ã¢m hoáº·c dÆ°Æ¡ng)
    const critic = tf.layers.dense({units: 1}).apply(dense);
    
    // Táº¡o model vá»›i 1 Ä‘áº§u vÃ o vÃ  2 Ä‘áº§u ra
    return tf.model({inputs: input, outputs: [actor, critic]});
}

// Khá»Ÿi táº¡o mÃ´ hÃ¬nh, optimizer vÃ  mÃ´i trÆ°á»ng
const model = createActorCriticModel();
const optimizer = tf.train.adam(learningRate);  // Adam optimizer - adaptive learning rate
const env = new CartPole();

// ===========================
// 3. HÃ€M Váº¼ (RENDER) VÃ€ REPLAY EPISODE Tá»T NHáº¤T
// ===========================

// Láº¥y reference Ä‘áº¿n canvas vÃ  context Ä‘á»ƒ váº½
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');

// CÃ¡c tham sá»‘ váº½ (scale Ä‘á»ƒ chuyá»ƒn Ä‘á»•i tá»« khÃ´ng gian váº­t lÃ½ sang pixel)
const scale = 50;           // Tá»· lá»‡: 1 Ä‘Æ¡n vá»‹ váº­t lÃ½ = 50 pixel
const cartWidth = 50;       // Chiá»u rá»™ng xe (pixel)
const cartHeight = 30;      // Chiá»u cao xe (pixel)

/**
 * Váº½ tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a CartPole lÃªn canvas
 * @param {Array} state - Máº£ng [x, x_dot, theta, theta_dot] mÃ´ táº£ tráº¡ng thÃ¡i
 */
function render(state) {
    // XÃ³a canvas trÆ°á»›c khi váº½ frame má»›i
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    // Chuyá»ƒn Ä‘á»•i tá»a Ä‘á»™ x tá»« khÃ´ng gian váº­t lÃ½ sang pixel
    // canvas.width/2 lÃ  giá»¯a mÃ n hÃ¬nh (x=0 trong khÃ´ng gian váº­t lÃ½)
    const x = state[0] * scale + canvas.width / 2;
    const theta = state[2];  // GÃ³c nghiÃªng cá»§a gáº­y (radian)
    
    // Váº½ sÃ n (Ä‘Æ°á»ng ngang mÃ u xÃ¡m)
    ctx.beginPath();
    ctx.moveTo(0, 250);
    ctx.lineTo(600, 250);
    ctx.strokeStyle = '#888';
    ctx.lineWidth = 2;
    ctx.stroke();
    
    // Váº½ xe Ä‘áº©y (hÃ¬nh chá»¯ nháº­t mÃ u Ä‘en)
    ctx.fillStyle = '#555';
    ctx.fillRect(x - cartWidth/2, 250 - cartHeight/2, cartWidth, cartHeight);
    
    // Váº½ cÃ¢y gáº­y (Ä‘Æ°á»ng tháº³ng mÃ u Ä‘á»)
    const poleLen = 100;  // Chiá»u dÃ i gáº­y trÃªn mÃ n hÃ¬nh (pixel)
    // TÃ­nh tá»a Ä‘á»™ Ä‘áº§u gáº­y dá»±a trÃªn gÃ³c theta
    const poleX = x + Math.sin(theta) * poleLen;
    const poleY = 250 - Math.cos(theta) * poleLen;
    
    ctx.beginPath();
    ctx.moveTo(x, 250);           // Báº¯t Ä‘áº§u tá»« Ä‘á»‰nh xe
    ctx.lineTo(poleX, poleY);     // Káº¿t thÃºc á»Ÿ Ä‘áº§u gáº­y
    ctx.lineWidth = 8;
    ctx.lineCap = 'round';
    ctx.strokeStyle = '#d9534f';  // MÃ u Ä‘á»
    ctx.stroke();
}

/**
 * Chiáº¿u láº¡i (replay) episode tá»‘t nháº¥t Ä‘Ã£ lÆ°u
 * HÃ m nÃ y sáº½ váº½ tá»«ng frame má»™t cÃ¡ch tuáº§n tá»± Ä‘á»ƒ ngÆ°á»i xem tháº¥y Ä‘Æ°á»£c quÃ¡ trÃ¬nh chÆ¡i
 */
async function replayBestEpisode() {
    // Báº­t cá» replay Ä‘á»ƒ ngÄƒn training cháº¡y Ä‘á»“ng thá»i
    isReplaying = true;
    
    // Cáº­p nháº­t UI Ä‘á»ƒ hiá»ƒn thá»‹ Ä‘ang chiáº¿u láº¡i
    const mainStatus = document.getElementById('status-main');
    mainStatus.innerHTML = `ğŸ‰ REPLAYING BEST EPISODE: ${maxRewardSoFar.toFixed(0)} POINTS! ğŸ‰`;
    mainStatus.classList.add('highlight-record');  // ThÃªm hiá»‡u á»©ng highlight

    // Duyá»‡t qua tá»«ng frame Ä‘Ã£ lÆ°u vÃ  váº½ láº¡i tuáº§n tá»±
    for (let i = 0; i < bestEpisodeStates.length; i++) {
        render(bestEpisodeStates[i]);
        
        // DÃ¹ng Promise + setTimeout Ä‘á»ƒ táº¡o Ä‘á»™ trá»… giá»¯a cÃ¡c frame
        // 20ms tÆ°Æ¡ng Ä‘Æ°Æ¡ng khoáº£ng 50fps - tá»‘c Ä‘á»™ vá»«a Ä‘á»§ Ä‘á»ƒ máº¯t ngÆ°á»i nhÃ¬n tháº¥y
        // TÄƒng giÃ¡ trá»‹ nÃ y náº¿u muá»‘n chiáº¿u cháº­m hÆ¡n (vÃ­ dá»¥: 50ms = 20fps)
        await new Promise(r => setTimeout(r, 20)); 
    }

    // Táº¯t hiá»‡u á»©ng highlight vÃ  cá» replay
    mainStatus.classList.remove('highlight-record');
    isReplaying = false;
    
    // Tiáº¿p tá»¥c training ngay sau khi replay xong (náº¿u Ä‘ang trong cháº¿ Ä‘á»™ training)
    if (isTraining) {
        runEpisode();
    }
}

// ===========================
// 4. VÃ’NG Láº¶P HUáº¤N LUYá»†N CHÃNH (TRAINING LOOP)
// ===========================

/**
 * Cháº¡y má»™t episode huáº¥n luyá»‡n
 * Episode = má»™t láº§n chÆ¡i tá»« Ä‘áº§u Ä‘áº¿n khi fail hoáº·c Ä‘áº¡t Ä‘iá»ƒm tá»‘i Ä‘a
 * 
 * Quy trÃ¬nh:
 * 1. Thu tháº­p dá»¯ liá»‡u: chÆ¡i episode vÃ  lÆ°u láº¡i states, actions, rewards, values
 * 2. TÃ­nh returns: dÃ¹ng discount factor Ä‘á»ƒ tÃ­nh giÃ¡ trá»‹ ká»³ vá»ng
 * 3. Cáº­p nháº­t máº¡ng: dÃ¹ng gradient descent Ä‘á»ƒ cáº£i thiá»‡n policy vÃ  value function
 * 4. Kiá»ƒm tra ká»· lá»¥c: náº¿u cÃ³ ká»· lá»¥c má»›i thÃ¬ replay
 */
async function runEpisode() {
    // Kiá»ƒm tra Ä‘iá»u kiá»‡n: chá»‰ train náº¿u Ä‘ang báº­t training, khÃ´ng Ä‘ang replay vÃ  khÃ´ng Ä‘ang test play
    if (!isTraining || isReplaying || isTestPlaying) return;

    // Reset mÃ´i trÆ°á»ng vá» tráº¡ng thÃ¡i ban Ä‘áº§u
    let state = env.reset();
    let totalReward = 0;  // Tá»•ng Ä‘iá»ƒm tÃ­ch lÅ©y trong episode nÃ y
    
    // CÃ¡c máº£ng Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u cho viá»‡c training
    // Má»—i pháº§n tá»­ tÆ°Æ¡ng á»©ng vá»›i má»™t time step trong episode
    const stateBatch = [];   // CÃ¡c tráº¡ng thÃ¡i Ä‘Ã£ quan sÃ¡t
    const actionBatch = [];  // CÃ¡c hÃ nh Ä‘á»™ng Ä‘Ã£ thá»±c hiá»‡n
    const rewardBatch = [];  // CÃ¡c reward nháº­n Ä‘Æ°á»£c
    const valueBatch = [];   // CÃ¡c giÃ¡ trá»‹ do critic dá»± Ä‘oÃ¡n

    // ==========================================
    // PHASE A: THU THáº¬P Dá»® LIá»†U (DATA COLLECTION)
    // Cháº¡y á»Ÿ tá»‘c Ä‘á»™ cao, KHÃ”NG váº½ Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ training
    // ==========================================
    while (true) {
        // LÆ°u tráº¡ng thÃ¡i hiá»‡n táº¡i vÃ o batch
        // Quan trá»ng: pháº£i copy máº£ng (dÃ¹ng spread operator [...])
        // vÃ¬ state sáº½ bá»‹ thay Ä‘á»•i á»Ÿ bÆ°á»›c sau, náº¿u khÃ´ng copy thÃ¬ táº¥t cáº£ pháº§n tá»­ trong
        // stateBatch sáº½ cÃ¹ng trá» Ä‘áº¿n cÃ¹ng má»™t máº£ng vÃ  sáº½ bá»‹ cáº­p nháº­t giá»‘ng nhau
        stateBatch.push([...state]); 

        // DÃ¹ng tf.tidy() Ä‘á»ƒ tá»± Ä‘á»™ng giáº£i phÃ³ng bá»™ nhá»› TensorFlow
        // TrÃ¡nh memory leak khi lÃ m viá»‡c vá»›i TensorFlow.js
        const actionInfo = tf.tidy(() => {
            // Chuyá»ƒn state thÃ nh tensor 2D: shape = [1, 4]
            // Dimension Ä‘áº§u lÃ  batch size (1 vÃ¬ chá»‰ cÃ³ 1 state)
            const stateTensor = tf.tensor2d([state]);
            
            // MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n: actor cho xÃ¡c suáº¥t hÃ nh Ä‘á»™ng, critic cho giÃ¡ trá»‹
            const [probs, value] = model.predict(stateTensor);
            
            // Láº¥y dá»¯ liá»‡u tá»« tensor vá» JavaScript array
            const probData = probs.dataSync();
            
            // Chá»n hÃ nh Ä‘á»™ng theo policy (xÃ¡c suáº¥t)
            // probData[0] lÃ  xÃ¡c suáº¥t chá»n action 0 (trÃ¡i)
            // Náº¿u random < probData[0] thÃ¬ chá»n 0, ngÆ°á»£c láº¡i chá»n 1
            // ÄÃ¢y lÃ  sampling tá»« phÃ¢n phá»‘i xÃ¡c suáº¥t
            const action = Math.random() < probData[0] ? 0 : 1;
            
            // Tráº£ vá» hÃ nh Ä‘á»™ng Ä‘Ã£ chá»n vÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n
            return { 
                action: action, 
                value: value.dataSync()[0]  // GiÃ¡ trá»‹ lÃ  sá»‘ thá»±c
            };
        });

        // Láº¥y hÃ nh Ä‘á»™ng vÃ  giÃ¡ trá»‹ tá»« káº¿t quáº£ dá»± Ä‘oÃ¡n
        const { action, value } = actionInfo;
        
        // Thá»±c hiá»‡n hÃ nh Ä‘á»™ng trong mÃ´i trÆ°á»ng vÃ  nháº­n káº¿t quáº£
        const stepResult = env.step(action);
        
        // *** LÆ¯U Ã QUAN TRá»ŒNG: KhÃ´ng váº½ (render) á»Ÿ Ä‘Ã¢y Ä‘á»ƒ tÄƒng tá»‘c training ***
        // Viá»‡c váº½ canvas ráº¥t tá»‘n tÃ i nguyÃªn, chá»‰ váº½ khi replay episode tá»‘t nháº¥t

        // LÆ°u láº¡i dá»¯ liá»‡u vÃ o cÃ¡c batch
        actionBatch.push(action);
        rewardBatch.push(stepResult.reward);  // Reward: 1 náº¿u chÆ°a fail, 0 náº¿u fail
        valueBatch.push(value);

        // Cáº­p nháº­t tráº¡ng thÃ¡i cho bÆ°á»›c tiáº¿p theo
        state = stepResult.state;
        totalReward += stepResult.reward;
        
        // Äiá»u kiá»‡n káº¿t thÃºc episode:
        // - MÃ´i trÆ°á»ng bÃ¡o done (xe ra ngoÃ i hoáº·c gáº­y ngÃ£)
        // - Hoáº·c Ä‘áº¡t 2000 Ä‘iá»ƒm (giá»›i háº¡n Ä‘á»ƒ trÃ¡nh trÃ n bá»™ nhá»› náº¿u mÃ´ hÃ¬nh há»c quÃ¡ tá»‘t)
        if (stepResult.done || totalReward >= 2000) break; 
    }

    // ==========================================
    // PHASE B: TÃNH TOÃN RETURNS VÃ€ Cáº¬P NHáº¬T Máº NG
    // ==========================================
    
    // TÃ­nh discounted returns (giÃ¡ trá»‹ ká»³ vá»ng tá»« má»—i time step)
    // Return táº¡i time step t = reward_t + gamma * reward_{t+1} + gamma^2 * reward_{t+2} + ...
    // DÃ¹ng backward pass Ä‘á»ƒ tÃ­nh hiá»‡u quáº£: Ä‘i tá»« cuá»‘i episode vá» Ä‘áº§u
    const returns = [];
    let R = 0;  // Return tÃ­ch lÅ©y tá»« cuá»‘i episode
    
    // Duyá»‡t ngÆ°á»£c tá»« bÆ°á»›c cuá»‘i cÃ¹ng vá» bÆ°á»›c Ä‘áº§u tiÃªn
    for (let i = rewardBatch.length - 1; i >= 0; i--) {
        // Return táº¡i bÆ°á»›c i = reward táº¡i bÆ°á»›c i + gamma * return táº¡i bÆ°á»›c i+1
        R = rewardBatch[i] + gamma * R;
        // ThÃªm vÃ o Ä‘áº§u máº£ng (unshift) Ä‘á»ƒ giá»¯ thá»© tá»± thá»i gian
        returns.unshift(R);
    }

    // Cáº­p nháº­t trá»ng sá»‘ máº¡ng báº±ng gradient descent
    // tf.tidy() Ä‘á»ƒ tá»± Ä‘á»™ng cleanup tensor sau khi xong
    tf.tidy(() => {
        // Chuyá»ƒn Ä‘á»•i cÃ¡c batch thÃ nh tensor
        const stateTensor = tf.tensor2d(stateBatch);      // [num_steps, 4]
        const returnsTensor = tf.tensor1d(returns);       // [num_steps]
        const actionsTensor = tf.tensor1d(actionBatch, 'int32');  // [num_steps], kiá»ƒu integer
        
        // HÃ m loss Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a
        optimizer.minimize(() => {
            // Dá»± Ä‘oÃ¡n láº¡i policy vÃ  values tá»« states
            const [probs, values] = model.predict(stateTensor);
            
            // ===== CRITIC LOSS (Value Loss) =====
            // So sÃ¡nh giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vá»›i returns thá»±c táº¿
            // DÃ¹ng Mean Squared Error (MSE)
            // Má»¥c tiÃªu: critic há»c cÃ¡ch Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c giÃ¡ trá»‹ cá»§a tráº¡ng thÃ¡i
            const valueLoss = tf.losses.meanSquaredError(
                returnsTensor,           // GiÃ¡ trá»‹ thá»±c táº¿ (ground truth)
                values.reshape([-1])     // GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n, reshape vá» 1D
            );
            
            // ===== ADVANTAGE =====
            // Advantage = Returns - Value
            // Äo lÆ°á»ng hÃ nh Ä‘á»™ng tá»‘t hÆ¡n hay tá»‡ hÆ¡n so vá»›i ká»³ vá»ng
            // Advantage > 0: hÃ nh Ä‘á»™ng tá»‘t hÆ¡n ká»³ vá»ng -> nÃªn tÄƒng xÃ¡c suáº¥t
            // Advantage < 0: hÃ nh Ä‘á»™ng tá»‡ hÆ¡n ká»³ vá»ng -> nÃªn giáº£m xÃ¡c suáº¥t
            const advantage = returnsTensor.sub(values.reshape([-1]));
            
            // ===== ACTOR LOSS (Policy Loss) =====
            // Chuyá»ƒn actions thÃ nh one-hot encoding: [0] -> [1,0], [1] -> [0,1]
            const oneHotActions = tf.oneHot(actionsTensor, numActions);
            
            // Láº¥y xÃ¡c suáº¥t cá»§a hÃ nh Ä‘á»™ng Ä‘Ã£ chá»n
            // probs: [num_steps, 2] (xÃ¡c suáº¥t cho action 0 vÃ  1)
            // oneHotActions: [num_steps, 2] (1 á»Ÿ vá»‹ trÃ­ action Ä‘Ã£ chá»n)
            // mul: nhÃ¢n element-wise, sum(1): tá»•ng theo chiá»u hÃ nh Ä‘á»™ng -> Ä‘Æ°á»£c [num_steps]
            const selectedProbs = probs.mul(oneHotActions).sum(1);
            
            // TÃ­nh log probability (cáº§n cho policy gradient)
            // ThÃªm 1e-5 Ä‘á»ƒ trÃ¡nh log(0) = -Infinity (khi xÃ¡c suáº¥t = 0)
            const logProbs = selectedProbs.log().add(1e-5);
            
            // Policy Loss = -mean(log_prob * advantage)
            // Dáº¥u trá»« vÃ¬ ta muá»‘n maximize reward nÃªn minimize negative reward
            // NhÃ¢n vá»›i advantage Ä‘á»ƒ tÄƒng xÃ¡c suáº¥t hÃ nh Ä‘á»™ng tá»‘t, giáº£m hÃ nh Ä‘á»™ng xáº¥u
            const policyLoss = logProbs.mul(advantage).mean().mul(-1);
            
            // Tá»•ng loss = Policy Loss + Value Loss
            // CÃ¢n báº±ng giá»¯a viá»‡c há»c policy tá»‘t vÃ  Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c
            return policyLoss.add(valueLoss);
        });
    });

    // ==========================================
    // PHASE C: KIá»‚M TRA Ká»¶ Lá»¤C VÃ€ ÄIá»€U HÆ¯á»šNG
    // ==========================================
    
    episodeCount++;  // TÄƒng bá»™ Ä‘áº¿m episode
    
    // Láº¥y reference Ä‘áº¿n cÃ¡c pháº§n tá»­ UI Ä‘á»ƒ cáº­p nháº­t
    const mainStatus = document.getElementById('status-main');
    const subStatus = document.getElementById('status-sub');

    // LuÃ´n cáº­p nháº­t ká»· lá»¥c hiá»‡n táº¡i
    subStatus.innerText = `Current record: ${maxRewardSoFar.toFixed(0)}`;

    // Kiá»ƒm tra xem cÃ³ phÃ¡ ká»· lá»¥c khÃ´ng
    if (totalReward > maxRewardSoFar) {
        // ========== PHÃ Ká»¶ Lá»¤C Má»šI! ==========
        maxRewardSoFar = totalReward;
        
        // LÆ°u láº¡i toÃ n bá»™ chuá»—i tráº¡ng thÃ¡i cá»§a episode tá»‘t nháº¥t
        // Copy toÃ n bá»™ stateBatch Ä‘á»ƒ dÃ¹ng cho replay sau nÃ y
        bestEpisodeStates = [...stateBatch];
        
        // LÆ°u láº¡i model weights cá»§a mÃ´ hÃ¬nh tá»‘t nháº¥t
        // Clone model Ä‘á»ƒ cÃ³ báº£n sao Ä‘á»™c láº­p, khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi training tiáº¿p theo
        // Äiá»u nÃ y cho phÃ©p chÆ¡i thá»­ vá»›i policy tá»‘t nháº¥t ngay cáº£ khi mÃ´ hÃ¬nh hiá»‡n táº¡i Ä‘Ã£ thay Ä‘á»•i
        if (bestModel) {
            bestModel.dispose();  // Giáº£i phÃ³ng model cÅ© Ä‘á»ƒ trÃ¡nh memory leak
        }
        // Clone toÃ n bá»™ model vá»›i cÃ¹ng cáº¥u trÃºc
        bestModel = createActorCriticModel();
        // Copy weights tá»« model hiá»‡n táº¡i sang bestModel
        // getWeights() tráº£ vá» array cÃ¡c tensor, setWeights() sáº½ tá»± Ä‘á»™ng clone ná»™i bá»™
        const weights = model.getWeights();
        bestModel.setWeights(weights); 
        
        // Cáº­p nháº­t UI: thÃ´ng bÃ¡o ká»· lá»¥c má»›i vÃ  chuáº©n bá»‹ replay
        mainStatus.innerText = `New record Ep ${episodeCount}: ${totalReward.toFixed(0)} points. Preparing replay...`;
        mainStatus.classList.add('highlight-record');

        // Äá»£i 1 giÃ¢y rá»“i báº¯t Ä‘áº§u chiáº¿u láº¡i episode tá»‘t nháº¥t
        // Thá»i gian nÃ y Ä‘á»ƒ ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ Ä‘á»c thÃ´ng bÃ¡o
        setTimeout(() => {
            mainStatus.classList.remove('highlight-record');
            replayBestEpisode();  // Báº¯t Ä‘áº§u replay
        }, 1000);

    } else {
        // ========== CHÆ¯A PHÃ Ká»¶ Lá»¤C ==========
        // Hiá»ƒn thá»‹ thÃ´ng tin episode hiá»‡n táº¡i
        mainStatus.innerText = `Training Ep ${episodeCount}... Score: ${totalReward.toFixed(0)}`;
        
        // Tiáº¿p tá»¥c training episode tiáº¿p theo ngay láº­p tá»©c
        // setImmediate Æ°u tiÃªn hÆ¡n setTimeout nhÆ°ng khÃ´ng pháº£i trÃ¬nh duyá»‡t nÃ o cÅ©ng há»— trá»£
        // NÃªn dÃ¹ng fallback setTimeout(..., 0) náº¿u setImmediate khÃ´ng cÃ³
        if (isTraining) {
            (window.setImmediate || setTimeout)(runEpisode, 0);
        }
    }
}

// ===========================
// 4.5. HÃ€M CHÆ I THá»¬ (TEST PLAY)
// ===========================

/**
 * ChÆ¡i thá»­ má»™t episode vá»›i mÃ´ hÃ¬nh hiá»‡n táº¡i
 * KhÃ¡c vá»›i training: cÃ³ render Ä‘áº§y Ä‘á»§, khÃ´ng cáº­p nháº­t trá»ng sá»‘ máº¡ng
 * Cho phÃ©p ngÆ°á»i dÃ¹ng xem mÃ´ hÃ¬nh chÆ¡i nhÆ° tháº¿ nÃ o vá»›i policy hiá»‡n táº¡i
 */
async function playTestEpisode() {
    // Náº¿u Ä‘ang replay hoáº·c Ä‘ang test play khÃ¡c thÃ¬ bá» qua
    if (isReplaying || isTestPlaying) return;
    
    // Kiá»ƒm tra xem Ä‘Ã£ cÃ³ best model chÆ°a
    // Náº¿u chÆ°a cÃ³ episode tá»‘t nháº¥t thÃ¬ khÃ´ng thá»ƒ test play
    if (!bestModel) {
        const mainStatus = document.getElementById('status-main');
        mainStatus.innerText = "No best model yet. Train more to get a record first!";
        return;
    }
    
    // Báº­t cá» test play vÃ  táº¡m dá»«ng training
    isTestPlaying = true;
    const wasTraining = isTraining;  // LÆ°u láº¡i tráº¡ng thÃ¡i training trÆ°á»›c Ä‘Ã³
    isTraining = false;              // Táº¡m dá»«ng training
    
    // Cáº­p nháº­t UI vÃ  disable button Ä‘á»ƒ trÃ¡nh click nhiá»u láº§n
    const mainStatus = document.getElementById('status-main');
    const subStatus = document.getElementById('status-sub');
    const testBtn = document.getElementById('btn-test');
    testBtn.disabled = true;
    testBtn.innerText = "Playing...";
    
    mainStatus.innerText = `Test Play: Playing with BEST model (Record: ${maxRewardSoFar.toFixed(0)} points)...`;
    subStatus.innerText = "Using the best policy learned so far.";
    
    // Táº¡o mÃ´i trÆ°á»ng má»›i Ä‘á»ƒ test (khÃ´ng dÃ¹ng env chung vá»›i training)
    const testEnv = new CartPole();
    let state = testEnv.reset();
    let totalReward = 0;
    let stepCount = 0;
    
    // Cháº¡y episode vá»›i render Ä‘áº§y Ä‘á»§
    while (true) {
        // Render tráº¡ng thÃ¡i hiá»‡n táº¡i
        render(state);
        
        // Dá»± Ä‘oÃ¡n hÃ nh Ä‘á»™ng tá»« BEST MODEL (policy tá»‘t nháº¥t) thay vÃ¬ model hiá»‡n táº¡i
        // Äiá»u nÃ y cho phÃ©p xem mÃ´ hÃ¬nh tá»‘t nháº¥t chÆ¡i nhÆ° tháº¿ nÃ o
        const actionInfo = tf.tidy(() => {
            const stateTensor = tf.tensor2d([state]);
            const [probs] = bestModel.predict(stateTensor);  // DÃ¹ng bestModel thay vÃ¬ model
            const probData = probs.dataSync();
            
            // Chá»n hÃ nh Ä‘á»™ng theo policy (sampling)
            const action = Math.random() < probData[0] ? 0 : 1;
            return { action };
        });
        
        // Thá»±c hiá»‡n hÃ nh Ä‘á»™ng
        const stepResult = testEnv.step(actionInfo.action);
        state = stepResult.state;
        totalReward += stepResult.reward;
        stepCount++;
        
        // Cáº­p nháº­t UI vá»›i Ä‘iá»ƒm sá»‘ hiá»‡n táº¡i
        mainStatus.innerText = `Test Play: Score ${totalReward} (${stepCount} steps)`;
        
        // Äá»£i má»™t chÃºt Ä‘á»ƒ animation mÆ°á»£t mÃ  (20ms = 50fps)
        await new Promise(r => setTimeout(r, 20));
        
        // Kiá»ƒm tra Ä‘iá»u kiá»‡n káº¿t thÃºc: chá»‰ dá»«ng khi thua (done = true)
        // KhÃ´ng giá»›i háº¡n Ä‘iá»ƒm sá»‘, Ä‘á»ƒ xem mÃ´ hÃ¬nh cÃ³ thá»ƒ chÆ¡i Ä‘Æ°á»£c bao lÃ¢u
        if (stepResult.done) {
            break;
        }
    }
    
    // Hiá»ƒn thá»‹ káº¿t quáº£ cuá»‘i cÃ¹ng
    mainStatus.innerText = `Test Play Finished: ${totalReward.toFixed(0)} points in ${stepCount} steps`;
    subStatus.innerText = `Current record: ${maxRewardSoFar.toFixed(0)}`;
    
    // Re-enable button
    testBtn.disabled = false;
    testBtn.innerText = "Test Play";
    
    // Äá»£i 2 giÃ¢y Ä‘á»ƒ ngÆ°á»i dÃ¹ng xem káº¿t quáº£
    await new Promise(r => setTimeout(r, 2000));
    
    // Táº¯t cá» test play vÃ  khÃ´i phá»¥c tráº¡ng thÃ¡i training
    isTestPlaying = false;
    isTraining = wasTraining;
    
    // Náº¿u Ä‘ang trong cháº¿ Ä‘á»™ training thÃ¬ tiáº¿p tá»¥c training
    if (isTraining) {
        mainStatus.innerText = "Resuming training...";
        runEpisode();
    } else {
        mainStatus.innerText = "Training paused. Click 'Test Play' again or 'Resume Training'.";
    }
}

// ===========================
// 5. ÄIá»€U KHIá»‚N GIAO DIá»†N NGÆ¯á»œI DÃ™NG (UI CONTROLS)
// ===========================

/**
 * Báº­t/táº¯t cháº¿ Ä‘á»™ training (pause/resume)
 * ÄÆ°á»£c gá»i khi ngÆ°á»i dÃ¹ng click nÃºt "Pause" hoáº·c "Resume Training"
 */
function toggleTraining() {
    // Äáº£o tráº¡ng thÃ¡i training
    isTraining = !isTraining;
    
    // Láº¥y reference Ä‘áº¿n cÃ¡c pháº§n tá»­ UI
    const btn = document.getElementById('btn-toggle');
    const status = document.getElementById('status-main');
    
    if (isTraining) {
        // Chuyá»ƒn sang cháº¿ Ä‘á»™ Ä‘ang training
        btn.innerText = "Pause";
        status.innerText = "Resuming training...";
        
        // Chá»‰ cháº¡y episode má»›i náº¿u khÃ´ng Ä‘ang trong quÃ¡ trÃ¬nh replay
        // Náº¿u Ä‘ang replay thÃ¬ sáº½ tá»± Ä‘á»™ng tiáº¿p tá»¥c sau khi replay xong
        if (!isReplaying) {
            runEpisode();
        }
    } else {
        // Chuyá»ƒn sang cháº¿ Ä‘á»™ táº¡m dá»«ng
        btn.innerText = "Resume Training";
        status.innerText = "Training paused.";
        // KhÃ´ng cáº§n gá»i runEpisode() vÃ¬ runEpisode() sáº½ tá»± kiá»ƒm tra isTraining
        // vÃ  return ngay náº¿u isTraining = false
    }
}

// ===========================
// KHá»I Táº O VÃ€ Báº®T Äáº¦U CHÆ¯Æ NG TRÃŒNH
// ===========================

// Váº½ má»™t frame Ä‘áº§u tiÃªn ngay khi trang load
// Äiá»u nÃ y Ä‘áº£m báº£o canvas khÃ´ng bá»‹ tráº¯ng khi Ä‘á»£i TensorFlow.js khá»Ÿi táº¡o
render(env.reset());

// Äá»£i 1 giÃ¢y Ä‘á»ƒ:
// 1. TensorFlow.js cÃ³ thá»i gian load vÃ  khá»Ÿi táº¡o hoÃ n toÃ n
// 2. NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ tháº¥y tráº¡ng thÃ¡i ban Ä‘áº§u trÆ°á»›c khi training báº¯t Ä‘áº§u
setTimeout(runEpisode, 1000);
